---
title: 'Lab 3: Reducing Crime (DRAFT: Stage 1)'
author: "N. Akkineni, K. Hanna, A. Thorp"
date: "November 27, 2018"
output:
    pdf_document:
    toc: true
    toc_depth: 1
    fig_height: 3
    df_print: kable
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\tableofcontents 

<!--
\listoffigures
\listoftables
-->

# Introduction (Stage 1: Draft Report)

Our team has been hired to provide research for a local political campaign, which would like to understand the determinants of crime rates and to provide policy suggestions that are applicable to local government. We examine the provided dataset to determine if a model with causal interpretation is feasible. After examining the data, we detail four regression models and find that estimators related to variables used to operationalize the concept of certainty of punishment are directionally consistent and statistically siginificant. From this we draw a limited policy recommendation to adopt a model of community policing to improve trust and information flow to law enforcement. However, our policy recommendations are limited because omitted variable bias confounds our estimators. Should local officials desire more robust conclusions, we recommend involving data scientists in the data collection process to improve our ability to draw causal inference from our modeling process and thus be able to make robust policy recommendations. 

```{r, result='asis'}
library(knitr)
library(kableExtra)
suppressMessages(library(car))
suppressMessages(library(stargazer))
suppressMessages(library(lmtest))

crime <- read.csv('crime_v2.csv',header = TRUE, sep = ",")

# Convert columns to factors and logical.
crime$county <- as.factor(crime$county)
crime$year <- as.factor(crime$year)
crime$west <- as.logical(crime$west)
crime$central <- as.logical(crime$central)
crime$urban <- as.logical(crime$urban)
```
<!--
codebook <- read.csv('codebook.csv')

# Create a log of the dependent variable
crime$logcrmrte <- log(crime$crmrte)
crime$east <- !(crime$west | crime$central)

# Create an average of all weekly wages values. 
crime$avgwage = (crime$wcon + crime$wtuc + crime$wtrd + crime$wfir + 
                   crime$wser + crime$wmfg + crime$wfed + crime$wsta + crime$wloc)/9

# Possible transformation for prbconv
#crime$adjprbconv = crime$prbconv/max(crime$prbconv)

# Reorder to place logcrmte next to crmrte and east next to central
# It's unsafe to keep this in, but handy for viewing data
#crime <- crime[,c(1,2,3,26,4:13,27,14:25,28)]
-->

# Exploratory Data Analysis

# Data Summary

We were provided with a dataset that includes crime statistics from the North Carolina Department of Corrections' prison and probation files, demographic statistics taken from the decennial census, police data derived from FBI police agency data and wage data from the North Carolina Employment Security commission.  In all we were provided with 25 variables and 90 counties.

Some of the values in this dataset were calculated from other datasets and we found some characteristics in the dataset which may bring its veracity in to question and we have addressed those below and in our analyses. 

### Data Clean Up

#### Null Rows
The dataset contained an apostrophe 6 rows after the data which caused the csv reader to create 6 invalid rows.  We removed these rows as they contain no data. 
```{r}
# Delete the 6 empty observations at the end, including the row with the apostrophe.
# We can use complete.cases to do this as these 6 observations are the only incomplete observations. 
crime = crime[complete.cases(crime), ]

# Fix prbconv which is a factor rather than numeric due to the apostrophe
# Convert from factor to numeric 
crime$prbconv = as.numeric(as.character(crime$prbconv))
```

We found two identical observations for county 193.  There is no logical reason to have two identical observations in this cross-sectional dataset, so we feel removing one of these two observations can only improve the quality of our analysis. 
```{r}
# county 193 is duplicated, remove one
crime = crime[!duplicated(crime), ]
```


### Concerns about the data

#### prbarr (Probability of Arrest)
We found that county 115 contained a value of 1.09 in prbarr (probability of arrest) which is not possible.  We beleive this to be a labeling error, as it is a ratio used to approximate a probability, rather than a true probability.

#### prbconv (Probability of Conviction)
We found 10 observations with values greater than 1, which, again, is not a possible value for probability.  The documentation in the codebook specifies that "(t)he probability of conviction is proxied by the ratio of convictions to arrests", which leaves some ambiguity, however it is plausible to have values greater than 1 as a single arrest can result in multiple convictions and persons can be convicted in absentia. Similar to the prbarr, we beleive this to be a labeling error, as it is a ratio used to approximate a probability, rather than a true probability.

#### Omitted Counties Creating Bias

The dataset contained 90 counties, and there are 100 couties in North Carolina.  The observation id labeled 'county' in our data set appears to contain FIPS codes, if this assumption is correct the following are the missing counties.  

+ 29 - Camden County 
+ 31 - Carteret County 
+ 43 - Clay County 
+ 73 - Gates County 
+ 75 - Graham County 
+ 97 - Iredell County 
+ 103 - Jones County
+ 121 - Mitchell County
+ 177 - Tyrrell County
+ 199 - Yancey County

These missing counties will introduce a slight clustering bias into our analyses at a minimum, and possibly a more significant bias if they were ommitted based on specific criteria whether deliberate or not.

## Univariate Analysis

```{r}
quick_uni_analysis = function(variable, description, roundto = 8) {
  hist(variable, xlab = paste(tools::toTitleCase(description),
        paste('\n Shapiro:',
        round(as.numeric(shapiro.test(variable)[2]), roundto)
        )), main = "")
  
hist(log(variable), 
     xlab = tools::toTitleCase(paste('Log of', description,
            paste('\n Shapiro:', round(as.numeric(shapiro.test(log(variable))[2]), roundto)
            ))),  main = "", ylab = '')
}

```
<!-- quick_uni_analysis_new = function(variable, description, roundto = 8) {
  hist(variable, xlab = tools::toTitleCase(description), main = "")
  hist(log(variable), xlab = tools::toTitleCase(paste("Log of", description)), main = "", ylab = '')
}  



#  var_shpr <- shapiro.test(variable)[2]
#  var_log_shpr <- shapiro.test(log(variable))[2]
#  table_out <- data.frame(shptst = var_shpr, shplogtst = var_log_shpr)
#  colnames(table_out) <- c('Shapiro test', 'Shapiro test log')

#  kable(table_out)
  #kable(table_out, "latex", longtable = TRUE, booktabs = TRUE, caption = "") %>%
  #kable_styling(full_width = TRUE, latex_options = c("HOLD_position", "striped", "repeat_header"), row_label_position = 1) -->

### Key Variable

#### Crimes Committed Per Person

**Campaign Significance**: The political campaign which hired us is interested in policy prescriptions derived from causal analysis of crime rates.  This is the key variable our models will attempt to explain.
```{r, fig.height=3, fig.show='hold'}
par(mfrow=c(1,2))
quick_uni_analysis(crime$crmrte, 'crimes committed per person')
```
Crimes committed per capita has a fairly strong positive skew, applying a natural log transformation creates a more symmetrical distribution and results in a Shapiro-Wilk test p-value that we cannot reject. The transformed variable is preferable for modelling. 

### Explanatory Variables

#### Diagrams of Key Variables With and Without Log Transformations
```{r, fig.height=10, fig.fullwidth = TRUE}
par(mfrow=c(5,4))
quick_uni_analysis(crime$prbarr, 'Probability of Arrest', roundto = 10)
quick_uni_analysis(crime$prbconv, 'Probability of Conviction', roundto = 10)

quick_uni_analysis(crime$prbpris, 'Probability of Prison')
quick_uni_analysis(crime$avgsen, 'Average Sentence')

quick_uni_analysis(crime$polpc, 'Police as Per. of Pop.', roundto = 15)
quick_uni_analysis(crime$pctymle, 'Per. of Pop. That Are Young Males', roundto = 15)

quick_uni_analysis(crime$density, 'people per sq. mile', roundto = 14)
quick_uni_analysis(crime$taxpc, 'tax revenue per capita', roundto = 16)

quick_uni_analysis(crime$mix, 'Face-to-face offences to other', roundto = 9)
quick_uni_analysis(crime$pctmin80, 'perc. minority, 1980')
par(mfrow=c(1,1)) #Reset
```
<!--
```{r, fig.height=10, fig.fullwidth = TRUE}
#par(mfrow=c(3,3))
#boxplot(crime$prbarr, xlab = 'Probability of Arrest', notch = TRUE)
```
-->

#### Probability of Arrest

Probability of Arrest has a positive skew, applying a natural log transformation creates a more symmetrical distribution and results in a Shapiro-Wilk test p-value that we cannot reject the null hypothesis of normality. The transformed variable is preferable for modelling.

#### Probability of Conviction

The log transform is preferable - both for interpretation and for better adhering to modeling assumptions. However, even the logged version fails a Shapiro-Wilk normality test. Something to keep in mind.

#### Probability of Prison

From an interpretation standpoint, the logged version is preferable, although from an modeling assumption standpoint, the unlogged version is preferable. 

#### Average Sentence

The logged version is preferable from both an interpretation and modeling assumption standpoint.

#### Police as a Percentage of Population

Both logged and un-logged versions of police as a percentage of the population are non-normal. Neither is inherently preferable from a modeling assumptions standpoint. 

<!--
I agree with Alex now, this is redunant with the omitted variable secion on this variable.

The number of police can conceivably be either the cause of or result of crime rates.  However, there is a fair amount of research showing increasing the police workforce causes a reduction in crime according to Vollaard & Hamad $^{(1)}$ .  Without timeseries data with randomized changes to Police as a Percentage of Population we cannon determine the causal impact of increasing or decreasing police.  However, many other studies show that increasing the size of a police force does reduce crime.
-->
<!-- 
http://www.princeton.edu/~smello/papers/cops.pdf 
-->

Both logged and un-logged versions of the percent of population that is young and male are non-normal. Neither is inherently preferable from a modeling assumptions standpoint. $^{(1)}$ The Journal of Law & Economic https://www.jstor.org/stable/10.1086/666614 

#### Percentage of Population That Are Young Males

This variable has a strong positive skew, using a natural log transformation results in a distribution that is still skewed, however, it is closer to normal.

#### People per Square Mile

<!-- TODO -->
 ------------------ ** Naga and Alex ** ------------------   

This variable becomes less normal after log transformation, as skew shifts to the negative and becomes more extreme.  We elect to keep the untransformed variable for our regressions. 

#### Tax Revenue per Capita

Tax revenue per capita has a strong positive skew, using a natural log transformation results in a distribution that is still skewed, however, it is closer to normal.

#### Face-to-face offences to Other (Offence Mix)

This is a ratio of face-to-face crimes to all other crimes.  Face-to-face crimes include violent crimes and those with a higher probability of violence. 

**Campaign Significance**: Violent crimes create fear and fear is a strong motivator for voters.

The mix of face-to-face crimes to other crimes has a positive skew, applying a natural log transformation creates a more symmetrical distribution. However, the resulting Shapiro-Wilk test would still reject the null hypothesis of normality.  That said, the log transformation is preferable for modelling.

#### Percentage Minority, 1980

** Same issue as density **

# Model Analysis
Transforming the variables and storing them in the data frame

```{r}
crime$log_crmrte <- log(crime$crmrte)
crime$log_prbarr <- log(crime$prbarr)
crime$log_prbconv <- log(crime$prbconv)
crime$log_prbpris <- log(crime$prbpris)
crime$log_avgsen <- log(crime$avgsen)
crime$log_polpc <- log(crime$polpc)
crime$log_density <- log(crime$density)
crime$log_taxpc <- log(crime$taxpc)
crime$log_pctmin80 <- log(crime$pctmin80)
crime$log_mix <- log(crime$mix)
crime$log_pctymle <- log(crime$pctymle)
```

## Model1 - Minimum Specification

We anticipate that the crime rate depends on certainty of punishment and severity of punishment. As such, our base model includes variables that attempt to operationalize those concepts.The probability of arrest and probability of punishment operationalize certainty of punishment, while probability of prison and sentence length operationalize severity of punishment.

$$
\begin{aligned}
log(crmrte) &= \beta_0 + \beta_1log(prbarr) + \beta_2log(prbconv) + \\
&\beta_3log(prbpris) +\beta_4log(avgsen) + u
\end{aligned}
$$


```{r}
model1 = lm(log(crmrte) ~ log(prbarr) + log(prbconv)+
              log(prbpris) +  log(avgsen), data = crime)
``` 
#### Assumption - Linear model 
```{r , fig.height=3, fig.width=4.5, fig.align='center', fig.show='hold'}
plot(model1, which=1, cex.sub=0.75)
```
From the residuals vs fitted plot, we don't see a non-linear relatonship, so we treat this as a valid assumption.

#### Assumption - Random Sampling
We do not have enough information about the sample selection to investigate this claim. We know that our dataset includes 90 counties and that North Carolina has only 100 counties, so our sample is close to approximating the population. However, we do not know how the counties in our dataset were selected. This uncertainty leads us to be more cautious in interpreting results from our models.

#### Assumption - Multicollinearity

```{r}
# Find the maximum Correlation
cor_model1 <- data.matrix(subset(crime,
           select = c("log_crmrte","log_prbarr","log_prbconv","log_prbpris","log_avgsen")))
cor_model1 = cor(cor_model1)
diag(cor_model1) = 0 # Zero the diagonal as we are uninterested in those.
max(cor_model1)
```
```{r}
vif(model1)
```
Based on pairwise correlation and variance inflation factors, we do not detect evidence of multicollinearity negatively impacting our specification.

#### Assumption - 4 Exogeneity (Zero Conditional Mean)
From the residuals vs fitted plot, the red line is very influenced by the outliers on the ends, though it remains largely centered on zero, so we treat this as most-likely a valid assumption.

#### Additional Assumptions 
#### Homoscedasticity
From the same residuals vs fitted plot, we see it is very scattered with extreme outliers. So, it is not easy to determine Homoscedasticity from this plot only. 

```{r}
bptest(model1)
```

```{r}
ncvTest(model1)
```

Both tests are showing small p-values showing that we have to reject the hypothesis. Homoscedasticity does not appear to be a valid assumption here indicating that our standard errors may not be used for inference. We will need to return to this in future analyses to examine our ability to use robust standard errors or other compensating techniques in order to make inference from our models. 

#### Normality of Residuals
```{r , fig.height=3, fig.width=3, fig.show='hold'}
plot(model1, which=2)
hist(model1$residuals,main="Model1 Residuals")

```
Other than a few outliers, the distribution is relatively normal for our given sample size. We treat this as a valid assumption.

#### Cook's Distance:
```{r, fig.height=3, fig.width=3, fig.show='hold'}
plot(model1, which=4)
plot(model1, which=5)

```
There are some influential values however cook's distance is within the bounds. 

#### Calculating AIC
```{r}
AIC(model1)
```
The AIC for this model is 110.0643.

## Model2 - Optimal Specification

In additional to the explanatory variables introduced in our #Model1, we have decided to include the following variables in the model.

$$
\begin{aligned}
log(crmrte) &=\beta_0 + \beta_1log(prbarr) + \beta_2log(prbconv) + \beta_3log(prbpris) + \\ 
&\beta_4log(avgsen)+ \beta_5log(polpc) + \beta_6log(taxpc) + \\ 
&\beta_7log(density)+\beta_8log(pctymle)+\beta_9log(pctmin80)+ u
\end{aligned}
$$
```{r}
model2 = lm(log(crmrte) ~ log(prbarr) + log(prbconv)+ log(prbpris) +  log(avgsen)
            + log(polpc) + log(taxpc)+ log(density) +  log(pctymle) + log(pctmin80), data = crime)
```
#### Checking if our Assumptions are valid in the model- 
#### Assumption - Linear model 

```{r , fig.height=3, fig.width=4.5, fig.align='center', fig.show='hold'}
plot(model2, which=1, cex.sub=0.75)
```
From the residuals vs fitted plot, we don't see a non-linear relatonship, so we treat this as a valid assumption.

#### Assumption - Random Sampling
Our views are identical to the previous specification.

#### Assumption - Multicollinearity

```{r}
# Find the maximum Correlation
cor_model2 <- data.matrix(subset(crime, 
   select = c("log_crmrte","log_prbarr","log_prbconv","log_prbpris",
    "log_avgsen",   "log_polpc","log_taxpc","log_density", "log_pctymle","log_pctmin80")))
cor_model2 = cor(cor_model2)
diag(cor_model2) = 0 # Zero the diagonal as we are uninterested in those.
max(cor_model2)
```
```{r}
vif(model2)
```
Based on pairwise correlation and variance inflation factors, we do not detect evidence of multicollinearity negatively impacting our specification.

#### Assumption - Exogeneity (Zero Conditional Mean)
From the residuals vs fitted plot, the red line is influenced by outliers on either end, however the zero conditional mean appears to hold.

#### Additional Assumptions - Homoscedasticity
From the same residuals Vs fitted plot, we see that it is scattered with extreme outliers. It is not easy to determine homoscedasticity from this plot only. 
Running some additional tests
```{r}
bptest(model2)
```
```{r}
ncvTest(model2)
```
Both tests are showing large p-values showing that we fail to reject the null hypothesis. 

####Normality of Residuals
```{r , fig.height=3, fig.width=3, fig.show='hold'}
plot(model2, which=2)
hist(model2$residuals,main="Model2 Residuals")
```
Other than a few outliers, the distribution is relatively normal for our given sample size, so we treat it as a valid assumption.

####Cook's Distance:
```{r, fig.height=3, fig.width=3, fig.show='hold'}
plot(model2, which=4)
plot(model2, which=5)
```
There are some influential values [24,51,79] however cook's distance is within bounds.

```{r}
AIC(model2)
```
The AIC for this model is 30.5825

## Model3 - Optimal-2 Specification [Best-Fit model]

After observing Model2 - We found that tax and percent male doesn't influence crime extensively, so we remove those in our best-fit model

$$
\begin{aligned}
log(crmrte) &= \beta_0 + \beta_1log(prbarr) + \beta_2log(prbconv) + \beta_3log(prbpris) + \\
&\beta_4log(avgsen)+ \beta_5log(polpc) + \beta_6log(density)+\beta_7log(pctmin80)+ u \\
\end{aligned}
$$
```{r }
model3 = lm(log(crmrte) ~ log(prbarr) + log(prbconv)+ log(polpc) +
              log(density)  + log(pctmin80) +log(prbpris) + log(avgsen) , data = crime)
```
#### Checking if our Assumptions are valid in the model- 
#### Assumption - Linear model 
```{r , fig.height=3, fig.width=4.5, fig.align='center', fig.show='hold'}
plot(model3, which=1, cex.sub=0.75)
```
From the residuals vs fitted plot, we don't see a non-linear relatonship, so we treat this as a valid assumption.

#### Assumption - Random Sampling
Our views are identical to the previous model.

#### Assumption - Multicollinearity
```{r}
# Find the maximum Correlation
cor_model3 <- data.matrix(subset(crime,
                                 select = c("log_crmrte","log_prbarr","log_prbconv", 
                                            "log_avgsen","log_polpc","log_density","log_pctmin80")))
cor_model3 = cor(cor_model3)
diag(cor_model3) = 0 # Zero the diagonal as we are uninterested in those.
max(cor_model3)
```
```{r}
vif(model3)
```
Based on pairwise correlation and variance inflation factors, we do not detect evidence of multicollinearity negatively impacting our specification.

#### Assumption - Exogeneity (Zero Conditional Mean)
From the residuals vs fitted plot, the red line is very influenced by the outlierson either end, however the zero conditional mean appears to hold.

#### Additional Assumptions - Homoscedasticity
From the same residuals vs fitted plot, we see it is scattered with extreme outliers. So, it is not easy to determine Homoscedasticity from this plot only. 
```{r}
bptest(model3)
```
```{r}
ncvTest(model3)
```
Both tests are showing insignificant p-values showing that we fail to reject the null hypothesis of homoskedasticity. 

#### Normality of Residuals
```{r , fig.height=3, fig.width=3, fig.show='hold'}
plot(model3, which=2)
hist(model3$residuals,main="Model3 Residuals")
```
Other than a few outliers, the distribution is relatively normal for our given sample size, so we treat this as a valid assumption.

#### Cook's Distance:
```{r, fig.height=3, fig.width=3, fig.show='hold'}
plot(model3, which=4)
plot(model3, which=5)
```
There are some influential values [6,51,79] however cook's distance is within the bounds.

####Calculating AIC
```{r}
AIC(model3)
```
The AIC for this model is 26.9529

## Model4 - Using all variables Specification

The team wanted to check the robustness of prior model specifications by creating a model with maximal inclusion from our dataset to see if estimated relationships shift.
$$
\begin{aligned}
log(crmrte) &= \beta_0 + \beta_1log(prbarr) + \beta_2log(prbconv) + \beta_3log(prbpris) + \\ 
  &\beta_4log(avgsen)+ \beta_5log(polpc) + \beta_6log(taxpc)+\beta_7log(west) + \\
  &\beta_8log(central) + \beta_9log(urban) + \beta_10log(pctmin80) +\beta_11log(wcon)+ \beta_12og(wtuc) + \\
  &\beta_13log(wtrd)+\beta_14log(wfir)+ \beta_15log(wser) + \beta_16log(wmfg) + \beta_17log(wfed) + \\
  &\beta_18log(wsta)+ \beta_19log(wloc) + \beta_20log(mix)+\beta_21log(density) + \beta_22log(pctymle) + u \\
\end{aligned}
$$

```{r}
model4 = lm(log(crmrte) ~ log(prbarr) + log(prbconv)+ log(prbpris) + 
              log(avgsen)  + log(polpc) +log(density) + log(taxpc) + 
              west+central + urban + log(pctmin80) + wcon + wtuc + 
              wtrd + wfir + wser + wmfg + wfed + wsta + wloc + 
              log(mix) + log(pctymle), data = crime)
```
#### Checking if our Assumptions are valid in the model- 
#### Assumption - Linear model 
```{r , fig.height=3, fig.width=4.5, fig.align='center', fig.show='hold'}
plot(model4, which=1, cex.sub=0.75)
```
#### Assumption - Random Sampling
Our views are identical to the previous model.

#### Assumption - Multicollinearity
```{r}
vif(model4)
```
Computing VIF indicates that there is no multicollinearity. 

#### Assumption - Exogeneity (Zero Conditional Mean)
From the residuals vs fitted plot, the red line is very influenced by the outliers on either end, but the zero conditional mean appears to hold.

#### Additional Assumptions - Homoscedasticity
From the same residuals Vs fitted plot, we see it is scattered with extreme outliers. So, it is not easy to determine Homoscedasticity from this plot only. 
```{r}
bptest(model4)
```
```{r}
ncvTest(model4)
```
Both tests are showing large p-values showing that we fail to accept the null hypothesis. 

#### Normality of Residuals
```{r, fig.height=3, fig.width=3, fig.show='hold'}
plot(model4, which=2)
hist(model4$residuals,main="Model4 Residuals")
```
Other than a few outliers, the distribution is relatively normal for our given sample size. 

#### Cook's Distance:
```{r, fig.height=3, fig.width=3, fig.show='hold'}
plot(model4, which=4)
plot(model4, which=5)
```
There are some influential values [25,79,84] however cook's distance is within bounds.
#### Calculating AIC
```{r}
AIC(model4)
```
The AIC for this model is 31.94569

```{r, results='asis'}
stargazer(model1, model2,model3, model4, type = "latex",
         report = "vc", # Don't report errors, since we haven't covered them
         title = "Linear Models Predicting Crime Rate",
         keep.stat = c("rsq", "n"),
         omit.table.layout = "n",
         column.labels=c("Not good","Good","Better","Using All"),
         dep.var.caption  = "Measuring Crime Rate",
         dep.var.labels   = "Crime Rate")
```
#### Model1 -
Only 41.6% of crime rate is being explained by our model. 
prbarr, prbconv are shown to have a negative effect on crime, while the other independent variables prbpris and avgsen have a postive effect on crime.

We do not want to overinterpret the model as assumptions of homoskedasticity appear to be violated. If we accept the model at face value, we would see that the estimators for the probability of arrest and probability of conviction are statistically significantly different from zero. 

#### Model 2 - 
78.4% of crime rate is being explained by this model. 
Polpc, Density and Pctmin80 appear to have a positive effect on crime. 
While other independent variables are estimated as having a negative effect on crime.

#### Model 3 -
78.3% of crime rate is being explained by this model. 

This says that crime rate decreases if more people are arrested, convicted, sentenced in prison 

#### Model 4 -
83.5% of crime rate is being explained by this model
Using all variables, we got a better AIC compared to Model 1 and very close to Model 2- but it is not a best fit model

#### Best-Fit Model
Based on our analysis - we found that our Model3 is the best fit model with low AIC value 26.9529. 

# Omitted Variables

In order to make valid policy recommendations, we need confidence that our estimated coefficients for policy-relevant variables are unbiased, statistically significant, and practically significant. Statistical software makes it quite easy to determine if there is a relationship between a given variable and the dependent variable that is statistically significantly different from zero - an area of analysis that we will expand upon in follow-ups to this piece. Practical significance of our estimates requires just one extra step to interpret the meaning of the estimate for each variable under consideration. Accounting for elements which could bias our estimates is more difficult and, to some degree, not a solvable problem.  

We only have observational data available. Moreover, we are not able to design or even infer experiments for our data generating process. As such, we are left to reason about counterfactuals, rather than conduct experiments to verify the implications of our model. Additionally, we have a flawed data collection process, which we also have no ability to correct for. Our desired population variables are by-in-large not included in the dataset we were provided. Some of these desired variables are practically or ethically unobservable. Others were operationalized in a flawed manner, with a negative impact on our ability to model relationships with a causal interpretation. We address some of these issues here.  

Our ideal model of the causes of the crime rate would be something like:

$$
\begin{aligned}
crime\_rate &= \beta_0 + \beta_1crty\_punish + \beta_2svrty\_punish + \beta_3poverty\_rate\ + \\ &\beta_4educ + \beta_5social\_cohesion + \beta_6weapon\_availability + \beta_7real\_wage\ + \\ &\beta_{8}low\_skill\_unemployment\_rate +\beta_{9}age\_15\_to\_30\_proportion\_population\ + \\ &\beta_{10}percent\_of\_population\_previously\_committed\_crime + \\
&\beta_{11}percent\_of\_population\_previously_imprisoned + ... + error
\end{aligned}
$$

Unfortunately, we are unable to observe virtually all of these concepts.  

Some concepts have been operationalized in our dataset. For example, certainty of punishment has been operationalized through three variables: 1) the percent of the population which are police, 2) the proportion of arrests to crimes, and 3) the proportion of convictions to arrest. This is among the most effective operationlizations in this dataset. Severity of punishment is also operationalized  through 1) the proportion of convictions that result in a prison sentence and 2) the average length of a prison sentence. Nominal wages are operationalized in the dataset with average wages for certain industry groupings. None of poverty rate, education, social cohesion, weapon availability, cost of living, or the low skill unemployment rate are operationalized within this dataset.  

Moreover, certain variables that are included in our dataset are likely correlated with many of our desired variables, but actually measure something distinct - introducing the possibility for model estimates based on those variables to be biased and thus misleading. For example, the pctmin80 variable measures the percent of a county that was minority in 1980 - 7 years prior to our other observations. Setting the time divergence aside and extrapolating from national trends in the U.S. in the 1980s, the percentage of a county that belongs to a minority class could be the result of *red lining*, a segregating practice which led to poverty and low-quality education, both positively correlated with crime rate. It may also exhibit a parabolic relation with social cohesion. If we were to include pctmin80 in our regression, we would expect the model estimate to be biased as we have not adjusted for the impacts of education, poverty, or social cohesion. Examining the impact of education alone on the estimator for pctmin80 - as education was likely negatively correlated with pctmin80, and we expect educated to be negatively related to the crime rate, the model's estimate of the impact of the percent of a county which was minority in 1980 would be upwardly biased. In other words, the estimator for pctmin80 in the underspecified model would imply a much larger relationship between pctmin80 and crime rate than actually exists.  

Similarly, our dataset contains a variable density which is likely correlated with two of our desired but unobserved explanatory variables: social cohesion and poverty. In practice, in the U.S. in the 1980s, we would expect social cohesion to be negatively correlated with density, while poverty may be positively correlated with density. We expect the beta for social cohesion to crime rate to be negative, while the beta for poverty to crime rate is expected to be positive. The impact of both of these omitted variables is that the model's estimate for density is likely upwardly biased. As with pctmin80, the model would again overestimate the impact of density on crime rate.  

Our ability to interpret the variable polpc in our dataset is also compromised by omitted variable bias. While we understand the idea that increased police presence should increase the certainty of punishment (more likely to be detected and more likely to be caught) ceteris paribus, in our current dataset, we do not have the ability to use polpc in this way. We are unable to observe the counterfactual of the same location with the same characteristics at the same point in time having more or less police. Rather, the variable in our dataset is the current level of police as a percent of the population. Given that we expect local governments to respond to increased crime by highering more police, our model is more likely to reflect that higher crime rate locations also have higher police concentrations. Given an alternate work environment where we could retrieve more data, we might think about attempting to compensate for this by locating police concentration and crime rate statistics for previous years, then using them to create variables for the percentage point change in police concentration, which we could use to explain a newly created variable for the percentage point change in crime rate for a given location. However, in their current single point in time forms, our model is likely to estimate the relationship between police percentage and crime rate as positive, thus providing a misleadign estimate for the relationship we would actually like to observe. 

Finally, our dataset contains several variables with nominal wages for certain industries. Including these in our model is likely to be somewhat misleading, producing biased estimators because these measures are not adjusted for cost of living. Said in other terms, each of the nominal wage indicators is likely positively correlated with our desired explanatory variable - real wages. Conceptually, we expect the relationship between real wages and crime rate to be negative, while the relationship between real wages and nominal wages is positive. As such our model's estimator for wages is likely to understate the impact of wages on crime rate. As such, these nominal wage variables are an imperfect proxy for the desired variable real wages


# Conclusion

We examined several models of crime rate and found a directionally consistent, statistically significant negative relationship for the probability of arrest and the probability of conviction on crime rate. As such, policies adopted should focus on increasing the certainty of punishment for committing crimes. One such policy could focus on improving information flow from local communities to police and judicial officials. A good model to build off of is community policing, where police focus on developing ties to the local community to build trust and thereby promote flow of needed information. 

That said, our ability to draw policy prescriptions from our models is limited due to notable omitted variable bias, which leads our model's estimators to be biased. These omitted variable biases are not possible to overcome while limited to the current data collection process. Should more work requiring causal inference be desired on these relationships in the future, we would seek input into the data collecting process in order to correct for some of our omitted variable biases. 

<!--
\newpage
# Appendix A: Codebook

```{r echo = FALSE, results = 'asis'}
#kable(codebook[, c(2,3,4)], "latex", longtable = TRUE, booktabs = TRUE, caption = "Crime Data Codebook") %>%
#  kable_styling(full_width = TRUE, latex_options = c("HOLD_position", "striped", "repeat_header"), row_label_position = 1)  
```
-->
