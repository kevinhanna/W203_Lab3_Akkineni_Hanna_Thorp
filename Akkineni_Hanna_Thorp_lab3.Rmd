---
title: 'Lab 3: Reducing Crime (DRAFT: Stage 1)'
author: "C. Akkineni, A. Thorp, K. Hanna"
date: "November 27, 2018"
output:
    pdf_document:
    toc: true
    toc_depth: 2
    fig_height: 3
    df_print: kable
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\tableofcontents 
<!--
\listoffigures
\listoftables
-->

# Introduction (Stage 1: Draft Report)

Our team has been hired to provide research for a local political campaign to help the campaign understand the determinants of crime rates and to provide policy suggestions that are applicable to local government. We examine the provided dataset to determine if a model with causal interpretation is feasible. After examining the data, we detail four regression models and find that estimators related to variables used to operationalize the concept of certainty of punishment are directionally consistent and statistically siginificant. From this we draw a limited policy recommendation to adopt a model of community policing to improve trust and information flow to law enforcement. However, our policy recommendations are limited because omitted variable bias confounds our estimators. Should local officials desire more robust conclusions, we recommend involving data scientists in the data collection process to improve our ability to draw causal inference from our modeling process and thus be able to make robust policy recommendations. 

```{r, result='asis'}
library(knitr)
library(kableExtra)
suppressMessages(library(car))
suppressMessages(library(stargazer))
suppressMessages(library(lmtest))

crime <- read.csv('crime_v2.csv')

# Convert columns to factors and logical.
crime$county <- as.factor(crime$county)
crime$year <- as.factor(crime$year)
crime$west <- as.logical(crime$west)
crime$central <- as.logical(crime$central)
crime$urban <- as.logical(crime$urban)
```
<!--
codebook <- read.csv('codebook.csv')

# Create a log of the dependent variable
crime$logcrmrte <- log(crime$crmrte)
crime$east <- !(crime$west | crime$central)

# Create an average of all weekly wages values. 
crime$avgwage = (crime$wcon + crime$wtuc + crime$wtrd + crime$wfir + 
                   crime$wser + crime$wmfg + crime$wfed + crime$wsta + crime$wloc)/9

# Possible transformation for prbconv
#crime$adjprbconv = crime$prbconv/max(crime$prbconv)

# Reorder to place logcrmte next to crmrte and east next to central
# It's unsafe to keep this in, but handy for viewing data
#crime <- crime[,c(1,2,3,26,4:13,27,14:25,28)]
-->

# Exploratory Data Analysis

##  Data Summary

We were provided with a dataset that included crime statistics from the North Carolina Department of Corrections prison and probation files, demographic statistic taken from census, police data computed using FBI police agency data and wage data from the North Carolina Employment Security commission.  In all we were provided with 25 variables and 90 counties.

Some of the values in this dataset were calculated from other datasets and we found some characteristics in the dataset which may bring its veracity in to question and we have addressed those below and in our analyses. 

### Data Clean Up

#### Null Rows
The dataset contained a an apostrophe 6 rows after the data which caused the csv reader to create 6 invalid rows.  We removed these rows as they contain no data. 
```{r}
# Delete the 6 empty observations at the end, including the row with the apostrophe.
# We can use complete.cases to do this as these 6 observations are the only incomplete observations. 
crime = crime[complete.cases(crime), ]

# Fix prbconv which is a factor rather than numeric due to the apostrophe
# Convert from factor to numeric 
crime$prbconv = as.numeric(as.character(crime$prbconv))
```

We found two identical observations for county 193.  There is no logical reason to have two identical observations in this cross-sectional dataset, so we feel removing one of these two observations can only improve the quality of our analysis. 
```{r}
# county 193 is duplidated, remove one
crime = crime[!duplicated(crime), ]
```


### Concerns about data

#### prbarr (Probability of Arrest)
We found that county 115 contained a value of 1.09 in prbarr (probability of arrest) which is not possible.  We beleive this to be a coding error, it is a ratio and not a probability.

#### prbconv (Probability of Conviction)
We found 10 observations with values greater than 1, which, again, is not a possible value for probability.  The documentation in the codebook specifies that "(t)he probability of conviction is proxied by the ratio of convictions to arrests", which leaves some ambiguity, however it is plausible to have values greater than 1 as a single arrest can result in multiple convictions.

#### Omitted Counties Creating Bias

The dataset contained 90 counties, and there are 100 couties in North Carolina.  The observation id labeled 'county' in our data set appears to contain FIPS codes, if this assumption is correct the following are the missing counties.  

+ 29 - Camden County 
+ 31 - Carteret County 
+ 43 - Clay County 
+ 73 - Gates County 
+ 75 - Graham County 
+ 97 - Iredell County 
+ 103 - Jones County
+ 121 - Mitchell County
+ 177 - Tyrrell County
+ 199 - Yancey County

These missing counties will introduce a slight clustering bias into our analyses at a minimum, and possibly a more significant bias if they were ommitted based on specific criteria whether deliberate or not.

## Univariate Analysis

```{r}
quick_uni_analysis = function(variable, description, roundto = 8) {
  hist(variable, xlab = paste(tools::toTitleCase(description),
        paste('\n Shapiro:',
        round(as.numeric(shapiro.test(variable)[2]), roundto)
        )), main = "")
  
hist(log(variable), 
     xlab = tools::toTitleCase(paste('Log of', description,
            paste('\n Shapiro:', round(as.numeric(shapiro.test(log(variable))[2]), roundto)
            ))),  main = "", ylab = '')
}

```
<!-- quick_uni_analysis_new = function(variable, description, roundto = 8) {
  hist(variable, xlab = tools::toTitleCase(description), main = "")
  hist(log(variable), xlab = tools::toTitleCase(paste("Log of", description)), main = "", ylab = '')
}  



#  var_shpr <- shapiro.test(variable)[2]
#  var_log_shpr <- shapiro.test(log(variable))[2]
#  table_out <- data.frame(shptst = var_shpr, shplogtst = var_log_shpr)
#  colnames(table_out) <- c('Shapiro test', 'Shapiro test log')

#  kable(table_out)
  #kable(table_out, "latex", longtable = TRUE, booktabs = TRUE, caption = "") %>%
  #kable_styling(full_width = TRUE, latex_options = c("HOLD_position", "striped", "repeat_header"), row_label_position = 1) -->

### Key Variable

#### Crimes Committed Per Person


**Campaign Significance**: Crime rate is a politicized and effects economy  

This is the key variable we will be explaining in terms of other variables in most of our modeling. 
```{r, fig.height=4}
par(mfrow=c(1,2))
quick_uni_analysis(crime$crmrte, 'crimes committed per per.')
```

Crimes committed per capita has a fairly strong positive skew, applying a natural log transformation creates a more symmetrical distribution and results in a Shapiro-Wilk test p-value that we cannot reject.  

The transformed variable is preferable for modelling. 

### Explanatory

#### Diagrams of Key Variables With and Without Log Transformations
```{r, fig.height=10, fig.fullwidth = TRUE}
par(mfrow=c(5,4))
quick_uni_analysis(crime$prbarr, 'Probability of Arrest', roundto = 10)
quick_uni_analysis(crime$prbconv, 'Probability of Conviction', roundto = 10)

quick_uni_analysis(crime$prbpris, 'Probability of Prison')
quick_uni_analysis(crime$avgsen, 'Average Sentence')

quick_uni_analysis(crime$polpc, 'Police as Per. of Pop.', roundto = 15)
quick_uni_analysis(crime$pctymle, 'Per. of Pop. That Are Young Males', roundto = 15)

quick_uni_analysis(crime$density, 'people per sq. mile', roundto = 14)
quick_uni_analysis(crime$taxpc, 'tax revenue per capita', roundto = 16)

quick_uni_analysis(crime$mix, 'Face-to-face offences to other', roundto = 9)
quick_uni_analysis(crime$pctmin80, 'perc. minority, 1980')
par(mfrow=c(1,1)) #Reset
```
<!--
```{r, fig.height=10, fig.fullwidth = TRUE}
#par(mfrow=c(3,3))
#boxplot(crime$prbarr, xlab = 'Probability of Arrest', notch = TRUE)
```
-->

#### Probability of Arrest

Probability of Arrest has a positive skew, applying a natural log transformation creates a more symmetrical distribution and results in a Shapiro-Wilk test p-value that we cannot reject.  

The transformed variable is preferable for modelling.

#### Probability of Conviction

Log is preferable - both for interpretation and for better adhering to modeling assumptions. However, even the logged version fails a Shapiro-Wilk normality test. Something to keep in mind.

#### Probability of Prison

From an interpretation standpoint, the logged version is preferable, although from an modeling assumption standpoint, the unlogged version is preferable. 

#### Average Sentence

The logged version is preferable from both an interpretation and modeling assumption standpoint.

#### Police as a Percentage of Population

Both logged and un-logged versions of police as a percentage of the population are non-normal. Neither is inherently preferable from a modeling assumptions standpoint. 

The number of police can conceivably be either the cause of or result of crime rates.  However, there is a fair amount of research showing increasing the police workforce causes a reduction in crime according to Vollaard & Hamad $^{(1)}$  <!-- TODO -->.  Without timeseries data including changes to Police as a Percentage of Population we cannon determine the true effects.  However, many studies show that increasing the size of a police force does reduce crime.
<!-- 
http://www.princeton.edu/~smello/papers/cops.pdf 
-->

Both logged and un-logged versions of the percent of population that is young and male are non-normal. Neither is inherently preferable from a modeling assumptions standpoint. 

$^{(1)}$ The Journal of Law & Economic https://www.jstor.org/stable/10.1086/666614 

#### Percentage of Population That Are Young Males

This variable has a strong positive skew, using a natural log transformation results in a still skewwed distirbution, however, it is closer to normal.

#### People per Square Mile

<!-- TODO -->
 ------------------ ** Naga and Alex ** ------------------   

This variable becomes less normal after log transformation, shifts skew to the negative, and more extreme.  I don't think we should be using the log of this.  We can change this before handing it in, after, or not at all. I'll update this later before handing in.



#### Tax Revenue per Capita

#### Face-to-face offences to Other (Offence Mix)

This is a ratio of face-to-face crimes to all other crimes.  Face-to-face crimes include violent crimes and those with a higher probability of violence, hense the more severe crimes.  Focusing resources which reduces this ratio along with the overall crime rate would be more beneficial.   

**Campaign Significance**: Violent crimes create fear and fear is a strong motivator for voters.

Mix of face-to-face crimes to other crimes has a positive skew, applying a natural log transformation creates a more symmetrical distribution however, the resultingShapiro-Wilk test would be rejected at 0.039.  That said, the log transformation is   

The transformed variable is preferable for modelling.

#### Percentage Minority, 1980





# Model Analysis

```{r}
crime$log_crmrte <- log(crime$crmrte)
crime$log_prbarr <- log(crime$prbarr)
crime$log_prbconv <- log(crime$prbconv)
crime$log_prbpris <- log(crime$prbpris)
crime$log_avgsen <- log(crime$avgsen)
crime$log_polpc <- log(crime$polpc)
crime$log_density <- log(crime$density)
crime$log_taxpc <- log(crime$taxpc)
crime$log_pctmin80 <- log(crime$pctmin80)
crime$log_mix <- log(crime$mix)
crime$log_pctymle <- log(crime$pctymle)
```

## Model1 - Minimum Specification

Crime-Determinants: we anticipate crime rate depends on average sentencing as higher crime rate tends to have higher sentencing. And Increased avgsen suggests there are severe crimes happening in a given county.
The two probability variables prbarr and prbconv will have strong correlations with prbpris and so we are including them as well in our model so that we can measure how much these variables influence crime rate.

$$
\begin{aligned}
log(crmrte) &= \beta_0 + \beta_1log(prbarr) + \beta_2log(prbconv) + \\
&\beta_3log(prbpris) +\beta_4log(avgsen) + u
\end{aligned}
$$


```{r}
model1 = lm(log(crmrte) ~ log(prbarr) + log(prbconv)+ log(prbpris) +  log(avgsen), data = crime)
```
### Checking if our Assumptions are valid in the model- 
Assumption 1 - Linear model -> the specified model has the dependent variable linear with explanatory variables 
```{r}
plot(model1, which=1)
```

from the Residuals vs Fitted plot, we don't see any non-linear relatonship.
So, it is a VALID Assumption.

Assumption - 2 Random Sampling
Since the dataset has counties information from only few regions it is not truly randomly sampled. But, we were provided with information that how the data is collected. Based on this 
we think it is a VALID Assumption.

Assumption -3 Multicollinearity

```{r}
cor_model1 <- data.matrix(subset(crime, select = c("log_crmrte","log_prbarr","log_prbconv","log_prbpris","log_avgsen")))
cor(cor_model1)
```
We are not seeing any obvious signs of multicollinearity. 

```{r}
vif(model1)
```

Computing VIF also indicates that there is no multicollinearity. 
So, it is a VALID Assumption.

Assumption - 4 Exogeneity (Zero Conditional Mean)
From the Residuals Vs Fitted Plot, the red line is very influenced by the outliers on the ends. 
It is a Most-Likely valid Assumption.

Additional Assumptions - Homoscedasticity
From the same Residuals Vs Fitted plot, we see it is very scattered with extreme outliers. So, it is not easy to determine Homoscedasticity from this plot only. 

```{r}
bptest(model1)
```

```{r}
ncvTest(model1)
```

Both tests are showing small p-values showing that we have to reject the hypothesis. 
So Homoscedasticity is not a valid assumption here indicating that our explanatory variables may not be not able to explain crime rate highly significant. 

Normality of Residuals
```{r}
plot(model1, which=2)
```

```{r}
hist(model1$residuals,main="Model1 Residuals")
```

Other than a few outliers, the distribution is relatively normal for our given sample size. 
So, it is a VALID assumption.

Cook's Distance:
```{r, fig.height=3, fig.width=3, fig.show='hold'}
plot(model1, which=4)
plot(model1, which=5)

```
There are some influential values however cook's distance is within the bounds

```{r}
AIC(model1)
```
The AIC for this model is 110.0643

```{r}
summary(model1)
```

Looking at the summary of model2, 38.98% of crime rate is being explained by our model. 
Looking at the co-efficient values -> prbarr, prbconv has negative effect on crime. 
While other independent variables prbpris and avgsen  has postive effect on crime.
Looking at the Pr(>|t|) values -> crime rate fluctuates more with prbarr, prbconv

This says that crime rate will decrease if people are arrested and convicted.


## Model2 - Optimal Specification

In additional to the explanatory variables intorudced in our #Model1, we have decided to include the following variables in the model.

Demographics - The team anticipates that crime behavior alters with demographics information such as race, gender, age. So, we are including pctymle and pctmin80 variables in our  model.

Density - The team is intereseted to see how density is altering crime  rates. The team expects that this should have a negative effect on crime.

Income Variables - The team expects that higher tax money means less crimes. Similar crimes would be less with better policing. So, we are inclduing polpc and taxpc variables in our model.


$$
\begin{aligned}
log(crmrte) &=\beta_0 + \beta_1log(prbarr) + \beta_2log(prbconv) + \beta_3log(prbpris) + \\ 
&\beta_4log(avgsen)+ \beta_5log(polpc) + \beta_6log(taxpc) + \\ 
&\beta_7log(density)+\beta_8log(pctymle)+\beta_9log(pctmin80)+ u
\end{aligned}
$$


```{r}
model2 = lm(log(crmrte) ~ log(prbarr) + log(prbconv)+ log(prbpris) +  log(avgsen)
            + log(polpc) + log(taxpc)+ log(density) +  log(pctymle) + log(pctmin80), data = crime)
```
### Checking if our Assumptions are valid in the model- 
Assumption 1 - Linear model -> the specified model has the dependent variable linear with explanatory variables 
```{r}
plot(model2, which=1)
```

from the Residuals vs Fitted plot, we don't see any non-linear relatonship.
So, it is a VALID Assumption.

Assumption - 2 Random Sampling
Since the dataset has counties information from only few regions it is not truly randomly sampled. But, we were provided with information that how the data is collected. Based on this 
we think it is a VALID Assumption.

Assumption -3 Multicollinearity

```{r}
cor_model2 <- data.matrix(subset(crime, select = c("log_crmrte","log_prbarr","log_prbconv","log_prbpris","log_avgsen",   "log_polpc","log_taxpc","log_density","log_pctymle","log_pctmin80")))
cor(cor_model2)
```
We are not seeing any obvious signs of multicollinearity. 

```{r}
vif(model2)
```

Computing VIF also indicates that there is no multicollinearity. 
So, it is a VALID Assumption.

Assumption - 4 Exogeneity (Zero Conditional Mean)
From the Residuals Vs Fitted Plot, the red line is very influenced by the outliers on the ends. But it is close to x-axis.
It is a Most-Likely valid Assumption.

Additional Assumptions - Homoscedasticity
From the same Residuals Vs Fitted plot, we see it is very scattered with extreme outliers. So, it is not easy to determine Homoscedasticity from this plot only. 

```{r}
bptest(model2)
```

```{r}
ncvTest(model2)
```

Both tests are showing small p-values showing that we fail to reject the hypothesis. 
So Homoscedasticity is a valid assumption here. 

Normality of Residuals
```{r}
plot(model2, which=2)
```

```{r}
hist(model2$residuals,main="Model2 Residuals")
```

Other than a few outliers, the distribution is relatively normal for our given sample size. 
So, it is a VALID assumption.

Cook's Distance:
```{r, fig.height=3, fig.width=3, fig.show='hold'}
plot(model2, which=4)
plot(model2, which=5)

```
There are some influential values [24,51,79] however cook's distance is within the bounds

```{r}
AIC(model2)
```
The AIC for this model is 30.50572

```{r}
summary(model2)
```

Looking at the summary of model2, 75.78% of crime rate is being explained by our  model. 
Looking at the co-efficient values -> Polpc, Density and Pctmin80 has a positive effect on crime. 
While other independent variables prbarr,prbconv,prbpris,avgsen,taxpc,pctymle has negative effect on crime.
Looking at the Pr(>|t|) values -> crime rate fluctuates more with prbarr, prbconv, density, pctmin80 and polpc
and has some impact with prbpris and avgsen

This says that crimerate decreases if more people are arrested, convicted, sentenced in prison 
With better policing in place more crimes would  be identified. And as density increases crimerates tend to go up. Also, Minority has a positive impact on  crime. 

## Model3 - Optimal-2 Specification

In additional to the explanatory variables intorudced in our #Model1, we have decided to include the following variables in the model.

Demographics - The team anticipates that crime behavior alters with demographics information such as race, gender, age. So, we are including pctymle and pctmin80 variables in our  model.

Density - The team is intereseted to see how density is altering crime  rates. The team expects that this should have a negative effect on crime.



$$
\begin{aligned}
log(crmrte) &= \beta_0 + \beta_1log(prbarr) + \beta_2log(prbconv) + \beta_3log(prbpris) + \\
&\beta_4log(avgsen)+ \beta_5log(polpc) + \beta_6log(density)+\beta_7log(pctmin80)+ u \\
\end{aligned}
$$


```{r}
model3 = lm(log(crmrte) ~ log(prbarr) + log(prbconv)+ log(polpc) +  log(density)  + log(pctmin80) +log(prbpris) + log(avgsen) , data = crime)
```
### Checking if our Assumptions are valid in the model- 
Assumption 1 - Linear model -> the specified model has the dependent variable linear with explanatory variables 
```{r}
plot(model3, which=1)
```

from the Residuals vs Fitted plot, we don't see any non-linear relatonship.
So, it is a VALID Assumption.

Assumption - 2 Random Sampling
Since the dataset has counties information from only few regions it is not truly randomly sampled. But, we were provided with information that how the data is collected. Based on this 
we think it is a VALID Assumption.

Assumption -3 Multicollinearity

```{r}
cor_model3 <- data.matrix(subset(crime, select = c("log_crmrte","log_prbarr","log_prbconv",  "log_polpc","log_density","log_pctmin80")))
cor(cor_model3)
```
We are not seeing any obvious signs of multicollinearity. 

```{r}
vif(model3)
```

Computing VIF also indicates that there is no multicollinearity. 
So, it is a VALID Assumption.

Assumption - 4 Exogeneity (Zero Conditional Mean)
From the Residuals Vs Fitted Plot, the red line is very influenced by the outliers on the ends. But it is close to x-axis.
It is a Most-Likely valid Assumption.

Additional Assumptions - Homoscedasticity
From the same Residuals Vs Fitted plot, we see it is very scattered with extreme outliers. So, it is not easy to determine Homoscedasticity from this plot only. 

```{r}
bptest(model3)
```

```{r}
ncvTest(model3)
```

Both tests are showing small p-values showing that we fail to reject the null hypothesis. 
So Homoscedasticity is a most likely valid assumption here. 

Normality of Residuals
```{r}
plot(model3, which=2)
```

```{r}
hist(model3$residuals,main="Model3 Residuals")
```

Other than a few outliers, the distribution is relatively normal for our given sample size. 
So, it is a VALID assumption.

Cook's Distance:
```{r, fig.height=3, fig.width=3, fig.show='hold'}
plot(model3, which=4)
plot(model3, which=5)

```
There are some influential values [6,51,79] however cook's distance is within the bounds

```{r}
AIC(model3)
```
The AIC for this model is 26.92602

```{r}
summary(model3)
```

Looking at the summary of model2, 76.26% of crime rate is being explained by our  model. 
Looking at the co-efficient values -> Polpc, Density and Pctmin80 has a positive effect on crime. 
While other independent variables prbarr,prbconv,prbpris,avgsen has negative effect on crime.
Looking at the Pr(>|t|) values -> crime rate fluctuates more with prbarr, prbconv, density, pctmin80 and polpc
and has some impact with prbpris and avgsen

This says that crimerate decreases if more people are arrested, convicted, sentenced in prison 
With better policing in place more crimes would  be identified. And as density increases crimerates tend to go up. Also, Minority has a positive impact on  crime. 

## Model4 - Using all variables Specification

The team wanted to check the robustness of all variables in the model except county name, year



$$
\begin{aligned}
log(crmrte) &= \beta_0 + \beta_1log(prbarr) + \beta_2log(prbconv) + \beta_3log(prbpris) + \\ 
  &\beta_4log(avgsen)+ \beta_5log(polpc) + \beta_6log(taxpc)+\beta_7log(west) + \\
  &\beta_8log(central) + \beta_9log(urban) + \beta_10log(pctmin80) +\beta_11log(wcon)+ \beta_12og(wtuc) + \\
  &\beta_13log(wtrd)+\beta_14log(wfir)+ \beta_15log(wser) + \beta_16log(wmfg) + \beta_17log(wfed) + \\
  &\beta_18log(wsta)+ \beta_19log(wloc) + \beta_20log(mix)+\beta_21log(density) + \beta_22log(pctymle) + u \\
\end{aligned}
$$


```{r}
model4 = lm(log(crmrte) ~ log(prbarr) + log(prbconv)+ log(prbpris) +  log(avgsen)  + log(polpc) +log(density) + log(taxpc) + west+central + urban + log(pctmin80)
            + wcon + wtuc + wtrd + wfir + wser + wmfg + wfed + wsta + wloc + log(mix) + log(pctymle), data = crime)
```
## Checking if our Assumptions are valid in the model- 
Assumption 1 - Linear model -> the specified model has the dependent variable linear with explanatory variables 
```{r}
plot(model4, which=1)
```

from the Residuals vs Fitted plot, we don't see any non-linear relatonship.
So, it is a VALID Assumption.

Assumption - 2 Random Sampling
Since the dataset has counties information from only few regions it is not truly randomly sampled. But, we were provided with information that how the data is collected. Based on this 
we think it is a VALID Assumption.

Assumption -3 Multicollinearity
```{r}
cor_model4 <- data.matrix(subset(crime, select = c("log_crmrte","log_prbarr","log_prbconv", "log_prbpris","log_avgsen", "log_polpc","log_density","log_taxpc", "west", "central", "urban", "log_pctmin80",
"wcon","wtuc","wtrd","wfir","wser","wmfg", "wfed", "wsta", "wloc", "log_mix", "log_pctymle")))
cor(cor_model4)
```
We are not seeing any obvious signs of multicollinearity. 

```{r}
vif(model4)
```

Computing VIF also indicates that there is no multicollinearity. 
So, it is a VALID Assumption.

Assumption - 4 Exogeneity (Zero Conditional Mean)
From the Residuals Vs Fitted Plot, the red line is very influenced by the outliers on the ends. But it is close to x-axis.
It is a Most-Likely valid Assumption.

Additional Assumptions - Homoscedasticity
From the same Residuals Vs Fitted plot, we see it is very scattered with extreme outliers. So, it is not easy to determine Homoscedasticity from this plot only. 

```{r}
bptest(model4)
```

```{r}
ncvTest(model4)
```

Both tests are showing small p-values showing that we fail to accept the hypothesis. 
So Homoscedasticity is a most likely valid assumption here. 

Normality of Residuals
```{r}
plot(model4, which=2)
```

```{r}
hist(model4$residuals,main="Model4 Residuals")
```

Other than a few outliers, the distribution is relatively normal for our given sample size. 
So, it is a VALID assumption.

Cook's Distance:
```{r, fig.height=3, fig.width=3, fig.show='hold'}
plot(model4, which=4)
plot(model4, which=5)

```
There are some influential values [25,79,84] however cook's distance is within the bounds

```{r}
AIC(model4)
```
The AIC for this model is 30.78825

```{r}
summary(model4)
```


Looking at the summary of model2, 78.26% of crime rate is being explained by our  model. 
Looking at the co-efficient values -> Polpc, Density and Pctmin80 has a positive effect on crime. 
While other independent variables prbarr,prbconv,prbpris,avgsen has negative effect on crime.
Looking at the Pr(>|t|) values -> crime rate fluctuates more with prbarr, prbconv, density, pctmin80 and polpc
and has some impact with prbpris and avgsen

This says that crimerate decreases if more people are arrested, convicted, sentenced in prison 
With better policing in place more crimes would  be identified. And as density increases crimerates tend to go up. Also, Minority has a positive impact on  crime. 


\newpage
# Omitted Variables

In order to make valid policy recommendations, we need confidence that our estimated coefficients for policy-relevant variables are unbiased, statistically significant, and practically significant. Statistical software makes it quite easy to determine if there is a relationship between a given variable and the dependent variable that is statistically significantly different from zero - an area of analysis that we will expand upon in follow-ups to this piece. Practical significance of our estimates requires just one extra step to interpret the meaning of the estimate for each variable under consideration. Accounting for elements which could bias our estimates is more difficult and, to some degree, not a solvable problem.  

We only have observational data available. Moreover, we are not able to design or even infer experiments for our data generating process. As such, we are left to reason about counterfactuals, rather than conduct experiments to verify the implications of our model. Additionally, we have a flawed data collection process, which we also have no ability to correct for. Our desired population variables are by-in-large not included in the dataset we were provided. Some of these desired variables are practically or ethically unobservable. Others were operationalized in a flawed manner, with a negative impact on our ability to model relationships with a causal interpretation. We address some of these issues here.  

Our ideal model of the causes of the crime rate would be something like:

$$
\begin{aligned}
crime\_rate &= \beta_0 + \beta_1crty\_punish + \beta_2svrty\_punish + \beta_3wealth\_inequality\ + \\ &\beta_4educ + \beta_5social\_cohesion + \beta_6weapon\_availability + \beta_7real\_wage\ + \\ &\beta_{8}low\_skill\_unemployment\_rate +\beta_{9}age\_15\_to\_30\_proportion\_population\ + \\ &\beta_{10}percent\_of\_population\_previously\_committed\_crime + 
\beta_{11}percent\_of\_population\_previously_imprisoned + ... + error
\end{aligned}
$$

Unfortunately, we are unable to observe virtually all of these concepts.  

Some concepts have been operationalized in our dataset. For example, certainty of punishment has been operationalized through three variables: 1) the percent of the population which are police, 2) the proportion of arrests to crimes, and 3) the proportion of convictions to arrest. This is among the most effective operationlizations in this dataset. Severity of punishment is also operationalized  through 1) the proportion of convictions that result in a prison sentence and 2) the average length of a prison sentence. Nominal wages are operationalized in the dataset with average wages for certain industry groupings. None of ealth inequality within a given observation, education, social cohesion, weapon availability, cost of living, or the low skill unemployment rate are operationalized within this dataset.  

Moreover, certain variables which are included in our dataset are likely correlated with many of our desired variables, but actually measure something distinct - introducing the possibility for model estimates based on those variables to be biased and thus misleading. For example, the pctmin80 variable measures the percent of a county that was minority in 1980 - 7 years prior to our other observations. Setting the time divergence aside and extrapolating from national trends in the U.S. in the 1980s, the percentage of a county which is minority is likely negatively correlated with education. It may also exhibit a parabolic relation with wealth inequality and social cohesion. If we were to include pctmin80 in our regression, we would expect the model estimate to be biased as we have not adjusted for the impacts of education, wealth inequality, or social cohesion. Examining the impact of education alone on the estimator for pctmin80 - as education was likely negatively correlated with pctmin80, and we expect educated to be negatively related to the crime rate, the model's estimate of the impact of the percent of a county which was minority in 1980 would be upwardly biased. In other words, the estimator for pctmin80 in the underspecified model would imply a much larger relationship between pctmin80 and crime rate than actually exists.  

Similarly, our dataset contains a variable density which is likely correlated with two of our desired but unobserved explanatory variables: social cohesion and wealth inequality. In practice, in the U.S. in the 1980s, we would expect social cohesion to be negatively correlated with density, while wealth inequality would be positively correlated with density. We expect the beta for social cohesion to crime rate to be negative, while the beta for wealth inequality to crime rate is expected to be positive. The impact of both of these omitted variables is that the model's estimate for density is likely upwardly biased. As with pctmin80, the model would again overestimate the impact of density on crime rate.  

Our ability to interpret the variable polpc in our dataset is also compromised by omitted variable bias. While we understand the idea that increased police presence should increase the certainty of punishment (more likely to be detected and more likely to be caught) ceteris paribus, in our current dataset, we do not have the ability to use polpc in this way. We are unable to observe the counterfactual of the same location with the same characteristics at the same point in time having more or less police. Rather, the variable in our dataset is the current level of police as a percent of the population. Given that we expect local governments to respond to increased crime by highering more police, our model is more likely to reflect that higher crime rate locations also have higher police concentrations. Given an alternate work environment where we could retrieve more data, we might think about attempting to compensate for this by locating police concentration and crime rate statistics for previous years, then using them to create variables for the percentage point change in police concentration, which we could use to explain a newly created variable for the percentage point change in crime rate for a given location. However, in their current single point in time forms, our model is likely to estimate the relationship between police percentage and crime rate as positive, thus providing a misleadign estimate for the relationship we would actually like to observe. 

Finally, our dataset contains several variables with nominal wages for certain industries. Including these in our model is likely to be somewhat misleading, producing biased estimators because these measures are not adjusted for cost of living. Said in other terms, each of the nominal wage indicators is likely positively correlated with our desired explanatory variable - real wages. Conceptually, we expect the relationship between real wages and crime rate to be negative, while the relationship between real wages and nominal wages is positive. As such our model's estimator for wages is likely to understate the impact of wages on crime rate. As such, these nominal wage variables are an imperfect proxy for the desired variable real wages

\newpage

# Conclusion

We examined several models of crime rate and found a directionally consistent, statistically significant negative relationship for the probability of arrest and the probability of conviction on crime rate. As such, policies adopted should focus on increasing the certainty of punishment for committing crimes. One such policy could focus on improving information flow from local communities to police and judicial officials. A good model to build off of is community policing, where police focus on developing ties to the local community to build trust and thereby promote flow of needed information. 

That said, our ability to draw policy prescriptions from our models is limited due to notable omitted variable bias, which leads our model's estimators to be biased. These omitted variable biases are not possible to overcome while limited to the current data collection process. Should more work requiring causal inference be desired on these relationships in the future, we would seek input into the data collecting process in order to correct for some of our omitted variable biases. 

<!--
\newpage
# Appendix A: Codebook

```{r echo = FALSE, results = 'asis'}
#kable(codebook[, c(2,3,4)], "latex", longtable = TRUE, booktabs = TRUE, caption = "Crime Data Codebook") %>%
#  kable_styling(full_width = TRUE, latex_options = c("HOLD_position", "striped", "repeat_header"), row_label_position = 1)  
```
-->
